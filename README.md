# üìÑ Informe T√©cnico: Simulaci√≥n "Minecraft RL Hardcore"

## 1. Resumen General
Este c√≥digo implementa una **simulaci√≥n multi-agente** compleja basada en el entorno del videojuego *Minecraft*. Utiliza la librer√≠a **SPADE** para la gesti√≥n de agentes y **Q-Learning** (Aprendizaje por Refuerzo) para dotar de inteligencia a los personajes ("Steves"). El objetivo es que los agentes aprendan aut√≥nomamente a sobrevivir a ciclos de d√≠a/noche, gestionar recursos, construir refugios y escapar de amenazas (Zombies).

## 2. Arquitectura del Sistema

### A. Tecnolog√≠as Clave
*   **Python Asyncio:** Gesti√≥n de concurrencia para el bucle del juego y servidores web.
*   **SPADE:** Framework para la creaci√≥n de agentes inteligentes (BDI/XMPP).
*   **HTML5/Canvas + JS:** Interfaz gr√°fica generada din√°micamente (`static/index.html`) para visualizar la simulaci√≥n en tiempo real.

### B. Entidades Principales
1.  **Steves (Agentes RL):** Agentes aut√≥nomos con salud, energ√≠a e inventario. Toman decisiones basadas en una tabla Q.
2.  **Zombies (NPCs):** Entidades hostiles con l√≥gica determinista (perseguir al m√°s cercano). Aumentan de nivel (tier) si matan.
3.  **ManagerAgent:** El "Dios" de la simulaci√≥n. Controla el tiempo, spawnea recursos/enemigos, gestiona la evoluci√≥n gen√©tica y sirve los datos a la web.
4.  **Objetos Pasivos:** Recursos (Comida, Madera, Piedra) y Casas (Refugios construibles/mejorables).

---

## 3. El Cerebro: Q-Learning (`QBrain`)

El n√∫cleo de la inteligencia reside en la clase `QBrain`. Los agentes no est√°n programados con reglas fijas (ej. "si tienes hambre, come"), sino que aprenden qu√© acci√≥n tomar seg√∫n su estado para maximizar una recompensa.

### üß† Definici√≥n de Estado (State)
El agente observa el mundo y simplifica su realidad en una cadena de texto (clave del diccionario Q):
*   **Tiempo:** `DAY` o `NIGHT`.
*   **Peligro:** `DANGER` (zombie cerca) o `SAFE`.
*   **Estado F√≠sico:** `DYING` (cr√≠tico), `TIRED` (poca energ√≠a) u `OK`.
*   **Inventario:** `HAS_FOOD`, `HAS_MATS` (materiales) o `POOR`.
*   **Ubicaci√≥n:** `INSIDE` (en casa) u `OUTSIDE`.

### ‚ö° Acciones Disponibles
`IDLE`, `EAT`, `SLEEP`, `GATHER_FOOD`, `GATHER_MATS`, `BUILD`, `REPAIR`, `FLEE`, `ENTER_HOUSE`.

### üíé Sistema de Recompensas (Rewards)
El aprendizaje se refuerza mediante premios y castigos num√©ricos:
*   **+80:** Construir una casa (gran incentivo).
*   **+25:** Dormir en casa de noche (recuperaci√≥n segura).
*   **+20:** Entrar a una casa o curarse comiendo.
*   **+5:** Sobrevivir la noche o recolectar recursos.
*   **-100:** Morir o recibir da√±o.
*   **-50:** Quedarse sin energ√≠a.

---

## 4. Din√°micas de la Simulaci√≥n

### Ciclo de Vida y Evoluci√≥n (Algoritmo Gen√©tico)
La simulaci√≥n ocurre por **Generaciones**:
1.  Si un agente muere, queda eliminado.
2.  Al finalizar un ciclo (d√≠a/noche), se seleccionan los supervivientes con mejor desempe√±o.
3.  **Herencia:** Los nuevos agentes de la siguiente generaci√≥n heredan la `Q-Table` (el cerebro aprendido) de los mejores supervivientes, acelerando el aprendizaje colectivo.
4.  Si todos mueren, se reinicia la simulaci√≥n ("Game Over").

### Interfaz Gr√°fica (GUI)
El c√≥digo genera un archivo `index.html` que se conecta v√≠a `fetch` al servidor Python.
*   **Visualizaci√≥n:** Muestra agentes (azul), zombies (verde), recursos y casas. Las barras de vida y acciones se dibujan sobre los personajes.
*   **Panel de Control:** Muestra estad√≠sticas en tiempo real (D√≠a, Generaci√≥n, Puntuaci√≥n Promedio) y permite controlar la velocidad de la simulaci√≥n.

---

## 5. Flujo de Ejecuci√≥n del C√≥digo

1.  **Inicializaci√≥n:** `ManagerAgent` levanta el servidor web en el puerto 10000 y crea la poblaci√≥n inicial (Gen 0).
2.  **Bucle (WorldLoop):**
    *   Verifica si es d√≠a o noche.
    *   Calcula l√≥gica de movimiento de Zombies.
    *   Solicita a cada Steve que perciba y act√∫e (`perceive_and_act`).
    *   Si es cambio de d√≠a, eval√∫a supervivientes y crea la nueva generaci√≥n.
3.  **Agente (Steve):**
    *   Consulta su `QBrain` con su estado actual.
    *   Ejecuta la acci√≥n (moverse, interactuar, etc.).
    *   Recibe recompensa inmediata.
    *   Actualiza su tabla Q (`brain.learn`).

## 6. Conclusi√≥n
Este script es un ejemplo robusto de **Aprendizaje por Refuerzo aplicado a supervivencia**. Combina la toma de decisiones individual (micro) con un sistema evolutivo (macro), permitiendo observar c√≥mo comportamientos complejos (como esconderse en casas por la noche) emergen de reglas simples y recompensas.

![Vista de la Simulaci√≥n](Picture.png)